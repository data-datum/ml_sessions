<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>representation.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">










background-image: url(img/diapo1.jpg)
background-size: cover
class: inverse,  middle


## Representation Learning on Graphs
### Machine Learning Sessions 
#### Roxana Noelia Villafañe

fecha: 2020-09-13

---
# Abstract


.panelset[
.panel[.panel-name[Graphs]

Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. 
]

.panel[.panel-name[Challenge]

The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models.
]

.panel[.panel-name[Embeddings]

Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. 
]

.panel[.panel-name[Aim]

**We provide a conceptual review of key advancements in this area of representation learning on graphs. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.**
]
]



---

# 1. Introduction

* Many machine learning applications seek to make predictions or discover new patterns using graph-structured data as feature information.
* The central problem in machine learning on graphs is finding a way to incorporate information about graph-structure into a machine learning model. 
* The challenge— from a machine learning perspective—is that there is no straightforward way to encode this high-dimensional, non-Euclidean information about graph structure into a feature vector.
* More recently, there has been a surge of approaches that seek to learn representations that encode structural information about the graph. **The idea behind these representation learning approaches is to learn a mapping that embeds nodes, or entire (sub)graphs, as points in a low-dimensional vector space. The goal is to optimize this mapping so that geometric relationships in the embedding space reflect the structure of the original graph.**

---

* Previous work treated this problem as a pre-processing step, using hand-engineered statistics to extract structural information. In contrast, **representation learning approaches treat this problem as machine learning task itself, using a data-driven approach to learn embeddings that encode graph structure.**

* Our survey attempts to **merge together multiple**, disparate lines of research that have drawn significant attention across different subfields and venues in recent years—e.g., *node embedding methods*, which are a popular object of study in the data mining community, and *graph convolutional networks*, which have drawn considerable attention in major machine learning venues. In doing so, **we develop a unified conceptual framework for describing the various approaches and emphasize major conceptual distinctions.**

---

# Assumptions



---

# Embedding nodes

#### The goal is to encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood.
#### These low- dimensional embeddings can be viewed as encoding, or projecting, nodes into a latent space, where geometric relations in this latent space correspond to interactions (e.g., edges) in the original graph.

&lt;img src="img/karate-club.png" width="90%" style="display: block; margin: auto;" /&gt;

---

### Overview of approaches: encoder-decoder perspective

* In this framework, we organize the various methods around two key mapping functions: **an encoder**, which maps each node to a low-dimensional vector, or embedding, and **a decoder**, which decodes structural information about the graph from the learned embeddings.

#### The intuition behind the encoder-decoder idea is the following: if we can learn to decode high-dimensional graph information—such as the global positions of nodes in the graph or the structure of local graph neighborhoods—from encoded low-dimensional embeddings, then, in principle, these embeddings should contain all information necessary for downstream machine learning tasks.

---
&lt;img src="img/encode-decode.png" width="100%" style="display: block; margin: auto;" /&gt;

**Once we have optimized the encoder-decoder system, we can use the trained encoder to generate embeddings for nodes, which can then be used as a feature inputs for downstream machine learning tasks.** For example, one could feed the learned embeddings to a logistic regression classifier to predict the community that a node belongs to [47], or one could use distances between the embeddings to recommend friendship links in a social network

---

### Methodological components

1. A pairwise similarity function sG : V ×V → R+, defined over the graph G. This function measures the similarity between nodes in G.
2. An encoder function, ENC, that generates the node embeddings. This function contains a number of trainable parameters that are optimized during the training phase.
3. A decoder function, DEC, which reconstructs pairwise similarity values from the generated embeddings. This function usually contains no trainable parameters.
4. A loss function, ?, which determines how the quality of the pairwise reconstructions is evaluated in order to train the model, i.e., how DEC(zi, zj) is compared to the true sG(vi, vj) values.

---

## Shallow-embedding approach

The majority of node embedding algorithms rely on what we call shallow embedding. For these shallow em- bedding approaches, the encoder function—which maps nodes to vector embeddings—is simply an “embedding lookup”:
`\(ENC(vi) = Zvi\)`
where `\(Z\)` ∈ `\(Rd×|V|\)` is a matrix containing the embedding vectors for all nodes and `\(vi\)` ∈ IV is a one-hot indicator vector indicating the column of `\(Z\)` corresponding to node `\(vi\)`. 


---

## Shallow embedding approaches

* **Factorization-based approaches**

1.  Laplacian eigenmaps
2.  Inner-product methods

--
* **Random walk approaches**

1. DeepWalk and node2vec
2. Large-scale information network embeddings (LINE)


---

### Major drawbacks of shallow embedding approaches

1. **No parameters are shared between nodes in the encoder** (i.e., the encoder is simply an embedding lookup based on arbitrary node ids). This can be statistically inefficient, since parameter sharing can act as a powerful *form of regularization*, and it is also computationally inefficient, since it means that the number of parameters in shallow embedding methods necessarily grows as `\(O(|V|)\)`.
2. **Shallow embedding also fails to leverage node attributes during encoding**. In many large graphs nodes have attribute information (e.g., user profiles on a social network) that is often highly informative with respect to the node’s position and role in the graph.
3. **Shallow embedding methods are inherently transductive**, i.e., they can only generate embeddings for nodes that were present during the training phase, and they cannot generate embeddings for previously unseen nodes unless additional rounds of optimization are performed to optimize the embeddings for these nodes. This is highly problematic for evolving graphs, massive graphs that cannot be fully stored in memory, or domains that require generalizing to new graphs after training.

---

## Generalized encoder-decoder architectures

* **Neighborhood autoencoder methods**

1. Deep Neural Graph Representations (DNGR)
2. Structural Deep Network embeddings (SDNE)

--

* **Neighborhood aggregation &amp; convolutional encoders**

---

## Neighborhood autoencoder methods

The basic idea behind these approaches is that they use autoencoders—a well known approach for deep learning in order to compress information about a node’s local neighborhood. DNGR and SDNE also differ from the previously reviewed approaches in that they use a *unary decoder* instead of a pairwise one.

--

&lt;img src="img/fig6.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## Neighborhood aggregation &amp; convolutional encoders

A number of recent node embedding approaches aim to solve the main limitations of the shallow embedding and autoencoder methods by designing encoders that **rely on a node’s local neighborhood, but not necessarily the entire graph**. The intuition behind these approaches is that they generate embeddings for a node by aggregating information from its local neighborhood
  
&lt;img src="img/fig7.png" width="100%" style="display: block; margin: auto;" /&gt;


---

#### Neighborhood aggregation &amp; convolutional encoders

* Unlike the previously discussed methods, these neighborhood aggregation algorithms rely on node features or attributes (denoted xi ∈ Rm) to generate embeddings. 

* For example, a social network might have text data (e.g., profile information), or a protein-protein interaction network might have molecular markers associated with each node. The neighborhood aggregation methods leverage this attribute information to inform their embeddings.
These methods are often called **convolutional because they represent a node as a function of its surrounding neighborhood, in a manner similar to the receptive field of a center-surround convolutional kernel in computer vision** 

---

### Algorithm (1)

In the encoding phase, the neighborhood aggregation methods build up the representation for a node in an iterative, or recursive, fashion. 

* First, the node embeddings are initialized to be equal to the input node attributes. 
* Then at each iteration of the encoder algorithm, nodes aggregate the embeddings of their neighbors, using an aggregation function that operates over sets of vectors.
* After this aggregation, every node is assigned a new embedding, equal to its aggregated neighborhood vector combined with its previous embedding from the last iteration. 
* Finally, this combined embedding is fed through a dense neural network layer and the process repeats. As the process iterates, the node embeddings contain information aggregated from further and further reaches of the graph.

As the process iterates, the node embeddings contain information aggregated from further and further reaches of the graph.

---

However, the dimensionality of the embeddings remains constrained as the process iterates, so the encoder is forced to compress all the neighborhood information into a low dimensional vector. After K iterations the process terminates and the final embedding vectors are output as the node representations.
              
**Graph Convolutional Networks (GCN), column networks, and the GraphSAGE algorithm follow the basic procedure outlined en 1.**


&lt;img src="img/pseudocode.png" width="100%" style="display: block; margin: auto;" /&gt;


---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
