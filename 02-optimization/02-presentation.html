<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>02-presentation.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">













background-image: url(img/diapo1.jpg)
background-size: cover
class: inverse,  middle


## Optimization in Neural Networks
### Machine Learning Sessions 
#### Roxana Noelia Villafañe

fecha: 2020-09-15

---
# Outline


* Generalities 
* Model Capacity 
* Batch size
* Loss Function


---
class: animated slideInRight fadeOutLeft


# Introduction 

* A neural network model uses the examples to learn how to map specific sets of input variables to the output variable. It must do this in such a way that this mapping works well for the training dataset, but also works well on new examples not seen by the model during training. This ability to work well on specific examples and new examples is called the ability of the model to generalize.


* A multilayer perceptron is just a mathematical function mapping some set of input
values to output values.


* Training error and generalization error generally differ: since the objective function of the optimization algorithm is usually a loss function based on the training dataset, the goal of optimization is to reduce the training error. However, the goal of statistical inference (and thus of deep learning) is to reduce the generalization error.





---
class: animated slideInRight fadeOutLeft

&lt;img src="img/error.png" width="50%" style="display: block; margin: auto;" /&gt;


.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
The goals of optimization and deep learning are fundamentally different. The former is primarily concerned with minimizing an objective whereas the latter is concerned with finding a suitable model.
]

---
class: animated slideInRight fadeOutLeft

### When talking about optimization in the context of neural networks, we are discussing non-convex optimization.

*Convex optimization* involves a function in which there is only one optimum, corresponding to the global optimum (maximum or minimum). There is no concept of local optima for convex optimization problems, making them relatively easy to solve — these are common introductory topics in undergraduate and graduate optimization classes.

*Non-convex optimization* involves a function which has multiple optima, only one of which is the global optima. Depending on the loss surface, it can be very difficult to locate the global optima.


---
class: animated slideInRight fadeOutLeft

.panelset[
.panel[.panel-name[Convex functions]

&lt;img src="img/convex.png" width="65%" style="display: block; margin: auto;" /&gt;
]

.panel[.panel-name[Non convex functions]

&lt;img src="img/nonconvex.png" width="70%" style="display: block; margin: auto;" /&gt;
]

.panel[.panel-name[Saddle points]

A flat region or saddle point is a point on the landscape where the gradient is zero. 
&lt;img src="img/saddle.png" width="70%" style="display: block; margin: auto;" /&gt;
]
]




---
class: animated slideInRight fadeOutLeft


# Navigating the Non-Convex Error Surface

* A change to the model weights will result in a change to the model error.

* The settling of the optimization process on a solution is referred to as **convergence**, as
the process has converged on a solution.

* This is a search or an optimization process and we refer to optimization algorithms that
operate in this way as gradient optimization algorithms, as they naively follow along the error gradient. The algorithm that is most commonly used to navigate the error surface is called **stochastic gradient descent**, or SGD for short.

* Stochastic Gradient Descent is more efficient as it uses the gradient information specifically to update the model weights via an algorithm called **backpropagation**.


---
class: animated slideInRight fadeOutLeft

* Backpropagation refers to a technique from calculus to calculate the derivative of the model error for specific model parameters, allowing model weights to be updated to move down the gradient.

## Components of the learning algorithm 

Training a deep learning neural network model using stochastic gradient descent with backpropagation involves choosing a number of components and hyperparameters, they are:

* Network Topology.
* Loss Function.
* Weight Initialization.
* Batch Size.
* Learning Rate.
* Epochs.
* Data Preparation.

---
class: animated slideInRight fadeOutLeft

# Network Topology

* The capacity of a neural network defines the scope of the mapping functions that the model can approximate. 
* A larger capacity means that the model is more flexible, but harder to train as it has many more parameters that have to be learned and provides a more challenging optimization problem to solve. 
* The number of nodes in the hidden layer define the capacity, and a network with a single hidden layer with a sufficient number of nodes can approximate any mapping function (so-called universal approximation). 



.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Network topology is the number of nodes (or equivalent) in the hidden layers and the
number of hidden layers in the network.

]
---
class: animated slideInRight fadeOutLeft

# Loss Function

An error function must be chosen, often called the objective function, cost function, or the loss function. Typically, a specific probabilistic framework for inference is chosen called Maximum Likelihood. Under this framework, the commonly chosen loss functions are cross-entropy for
classification problems and mean squared error for regression problems.



.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Loss Function is the function used to measure the performance of a model with a specific
set of weights on examples from the training dataset

]

---
class: animated slideInRight fadeOutLeft


# Weigth initialization

The search or optimization process requires a starting point from which to begin model
updates. The starting point is defined by the initial model parameters or weights. Because
the error surface is non-convex, the optimization algorithm is sensitive to the initial starting point. As such, small random values are chosen as the initial model weights, although different techniques can be used to select the scale and distribution of these values. These techniques are referred to as weight initialization methods. This can be tied to the choice of activation function.





.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Weight initialization is the procedure by which the initial small random values are
assigned to model weights at the beginning of the training process.


]

---
class: animated slideInRight fadeOutLeft

# Batch size

When updating the model, a number of examples from the training dataset must be used to
calculate the model error, often referred to simply as loss. All examples in the training dataset may be used, which may be appropriate for smaller datasets. Alternately, a single example may be used which may be appropriate for problems where examples are streamed or where the data changes often. A hybrid approach may be used where the number of examples from the training dataset may be chosen and used to used to estimate the error gradient. The choice of the number of examples is referred to as the batch size.





.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Batch Size is the number of examples used to estimate the error gradient before updating
the model parameters.

]


---
class: animated slideInRight fadeOutLeft


# Learning Rate

* Once an error gradient has been estimated, the derivative of the activation function can be calculated and used to update each parameter. There may be statistical noise in the training dataset and in the estimate of the error gradient. Also, the depth of the model (number of layers) and the fact that model parameters are updated separately means that it is hard to calculate exactly how much to change each model parameter to best way to move the whole model down the error surface. 

* Instead, a small portion of the update to the weights is performed each iteration. A hyperparameter called the learning rate controls how much to update model weights and, in turn, controls how fast a model learns on the training dataset.

.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Learning Rate is the amount that each model parameter is updated per iteration of the
learning algorithm.

]



---
class: animated slideInRight fadeOutLeft


# Epochs

* The training process must be repeated many times until a good or good enough set of
model parameters is discovered. The total number of iterations of the process is bounded by
the number of complete passes through the training dataset after which the training process
is terminated. This is referred to as the number of training epochs. This hyperparameter is
tightly related to both the choice of learning rate and batch size and can be set to a large value and almost ignored when using some regularization methods.

.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Epochs is the number of complete passes through the training dataset before the training
process is terminated.

]





---
class: animated slideInRight fadeOutLeft, inverse,  middle

background-image: url(img/diapo2.jpg)
background-size: cover

# Model Capacity 
---

class: animated slideInRight fadeOutLeft

# Model Capacity &amp; Network Topology

* Neural networks learn mapping functions. The capacity of a network refers to the range or scope of the functions that the model can approximate.

* Informally, a model’s capacity is its ability to fit a wide variety of functions.

* A model with less capacity may not be able to sufficiently learn the training dataset. A
model with more capacity can model more different functions and may be able to learn a
function to sufficiently map inputs to outputs in the training dataset. Whereas a model with too much capacity may memorize the training dataset and fail to generalize or get lost or stuck in the search for a suitable mapping function. Generally, we can think of model capacity as a control over whether the model is likely to underfit or overfit a training dataset.

**We can control whether a model is more likely to overfit or underfit by altering its
capacity.**



---
class: animated slideInRight fadeOutLeft


The capacity of a neural network can be controlled by two aspects of the model:
* Number of Nodes.
* Number of Layers.


A model with more nodes or more layers has a greater capacity and, in turn, is potentially
capable of navigating a larger set of mapping functions.




---

class: animated slideInRight fadeOutLeft,  middle

background-image: url(img/diapo3.jpg)
background-size: cover

# Batch size 

---

# Batch size

* Neural networks are trained using the stochastic gradient descent optimization algorithm. This involves using the current state of the model to make a prediction, comparing the prediction to the actual values, and using the difference as an estimate of the error gradient. 

* This error gradient is then used to update the model weights and the process is repeated. The error gradient is a statistical estimate. The more training examples used in the estimate, the more accurate this estimate will be and the more likely that the weights of the network will be adjusted in a way that will improve the performance of the model. 

* The improved estimate of the error gradient comes at the computational cost of having to use the model to make many more predictions before the estimate can be calculated, and in turn, the weights updated.

---

# Batch size

The number of training examples used in the estimate of the error gradient is a hyperparameter for the learning algorithm called the batch size, or simply the batch. 

A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated.

.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
* **Batch Gradient Descent**. Batch size is set to the total number of examples in the
training dataset.
* **Stochastic Gradient Descent.** Batch size is set to one.
* **Minibatch Gradient Descent.** Batch size is set to more than one and less than the
total number of examples in the training dataset.

]



---
class: animated slideInRight fadeOutLeft,  inverse, middle

background-image: url(img/diapo4.jpg)
background-size: cover

# Learning Rate

---

# Learning Rate

The weights of a neural network cannot be calculated using an analytical method. 

Instead, the weights must be discovered via an empirical optimization procedure called stochastic gradient descent. The optimization problem addressed by stochastic gradient descent for neural networks is challenging and the space of solutions (sets of weights) may be comprised of many good solutions (called global optima) as well as easy to find, but low in skill solutions (called local optima). 

The amount of change to the model during each step of this search process, or the step size, is called the learning rate and provides perhaps the most important hyperparameter to tune for your neural network in order to achieve good performance on your problem.

---

### Efect of the learning rate

* The learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated, such as at the end of each batch of training examples. Given a perfectly configured learning rate, the model will learn to best
approximate the function given available resources (the number of layers and the number of
nodes per layer) in a given number of training epochs (passes through the training data).

.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
* Generally, **a large learning rate allows the model to learn faster**, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.
]

* At extremes, a learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs.


---

## How to configure a learning rate

* The learning rate will interact with many other aspects of the optimization process, and the interactions may be nonlinear. Nevertheless, in general, smaller learning rates will require more training epochs. Conversely, larger learning rates will require fewer training epochs. Further, smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient. A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem.


* Unfortunately, we cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a good (or good enough) learning rate must be discovered via trial and error.

.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
The range of values to consider for the learning rate is less than 1.0 and greater than 10 −6.
]



---

# Momentum

* Training a neural network can be made easier with the addition of history to the weight update. Specifically, an exponentially weighted average of the prior updates to the weight can be included when the weights are updated. This change to stochastic gradient descent is called momentum and adds inertia to the update procedure, causing many past updates in one direction to continue in that direction in the future.

* Momentum can accelerate learning on those problems where the high-dimensional weight
space that is being navigated by the optimization process has structures that mislead the
gradient descent algorithm, such as flat regions or steep curvature.

* It has the effect of smoothing the optimization process, slowing updates to continue in the previous direction instead of getting stuck or oscillating.


.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Momentum is set to a value greater than 0.0 and less than one, where common values such
as 0.9 and 0.99 are used in practice.
Common values of [momentum] used in practice include .5, .9, and .99.
]



---

class: animated slideInRight fadeOutLeft,  inverse, middle

background-image: url(img/diapo5.jpg)
background-size: cover


# Adaptive Learning Rates
# (Optimizers)

---

# Adagrad 

* Adagrad decreases the learning rate dynamically on a per-coordinate basis.

* It uses the magnitude of the gradient as a means of adjusting how quickly progress is achieved - coordinates with large gradients are compensated with a smaller learning rate.

* Computing the exact second derivative is typically infeasible in deep learning problems due to memory and computational constraints. The gradient can be a useful proxy.

* If the optimization problem has a rather uneven uneven structure Adagrad can help mitigate the distortion.

* Adagrad is particularly effective for sparse features where the learning rate needs to decrease more slowly for infrequently occurring terms.

* On deep learning problems Adagrad can sometimes be too aggressive in reducing learning rates.

* For implementations from scratch http://d2l.ai/chapter_optimization/adagrad.html 

---

# RMSProp

* One of the key issues in Adagrad is that the learning rate decreases at a predefined schedule of effectively O(t−12)

* While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.

* [Tieleman &amp; Hinton, 2012] proposed the RMSProp algorithm as a simple fix to decouple rate scheduling from coordinate-adaptive learning rates.

* RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients.

* RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner.

* The learning rate needs to be scheduled by the experimenter in practice.

* For implementations from the scratch http://d2l.ai/chapter_optimization/rmsprop.html 

---

# Adadelta

* Adadelta is yet another variant of AdaGrad (Section 11.7). The main difference lies in the fact that it decreases the amount by which the learning rate is adaptive to coordinates. Moreover, traditionally it referred to as not having a learning rate since it uses the amount of change itself as calibration for future change. The algorithm was proposed in [Zeiler, 2012].

* Adadelta has no learning rate parameter. Instead, it uses the rate of change in the parameters itself to adapt the learning rate.

* Adadelta requires two state variables to store the second moments of gradient and the change in parameters.

* Adadelta uses leaky averages to keep a running estimate of the appropriate statistics.

* For implementations from the scratch http://d2l.ai/chapter_optimization/adadelta.html 

---

# Adam

* Adam [Kingma &amp; Ba, 2014] combines all these techniques into one efficient learning algorithm. As expected, this is an algorithm that has become rather popular as one of the more robust and effective optimization algorithms to use in deep learning. 

* In particular, [Reddi et al., 2019] show that there are situations where Adam can diverge due to poor variance control. In a follow-up work [Zaheer et al., 2018] proposed a hotfix to Adam, called Yogi which addresses these issues. 

* For implementations from the scratch http://d2l.ai/chapter_optimization/adam.html 

---

class: animated slideInRight fadeOutLeft, middle

background-image: url(img/diapo7.jpg)
background-size: cover


# Batch Normalization


---

# Batch Normalization 

* Training deep neural networks with tens of layers is challenging as they can be sensitive to the initial random weights and configuration of the learning algorithm. One possible reason for this difficulty is the distribution of the inputs to layers deep in the network may change after each minibatch when the weights are updated. This can cause the learning algorithm to forever chase a moving target. This change in the distribution of inputs to layers in the network is referred to by the technical name internal covariate shift.


.bg-lightest-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each minibatch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks. 
]





---
# Bibliography

* A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay https://arxiv.org/abs/1803.09820 

* Optimization algorithms http://d2l.ai/chapter_optimization/index.html 

* An overview of gradient descent optimization algorithms https://ruder.io/optimizing-gradient-descent/ 

* Why Momentum Really Works https://distill.pub/2017/momentum/ 

* Cyclical Learning Rates for Training Neural Networks https://arxiv.org/abs/1506.01186
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
